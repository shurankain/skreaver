name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential

    - name: Cache Cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-benchmark-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-benchmark-
          ${{ runner.os }}-cargo-

    - name: Create benchmark baseline directory
      run: mkdir -p target/baselines

    - name: Download previous benchmarks baseline
      uses: actions/cache@v4
      with:
        path: target/baselines
        key: benchmark-baselines-${{ github.repository }}-${{ github.ref_name }}
        restore-keys: |
          benchmark-baselines-${{ github.repository }}-main
          benchmark-baselines-${{ github.repository }}-

    - name: Run production benchmarks
      run: |
        cargo bench --bench production_benchmark 2>&1 | tee target/benchmark-output.txt
        echo "Benchmark completed successfully"

    - name: Generate benchmark report
      id: benchmark_report
      run: |
        # Create a simple benchmark analysis script
        cat > analyze_benchmarks.py << 'EOF'
        import json
        import sys
        import os

        def analyze_benchmarks():
            try:
                with open('target/benchmark-output.txt', 'r') as f:
                    content = f.read()
                    if not content:
                        print("No benchmark data available")
                        return
                    
                    print("Raw benchmark output:")
                    print(content)
                    
                    # Parse criterion text output for timing information
                    import re
                    performance_warning = False
                    
                    # Look for benchmark timing patterns in criterion output
                    # Pattern: benchmark_name    time:   [X.X ms Y.Y ms Z.Z ms]
                    pattern = r'(\w+)\s+time:\s+\[([0-9.]+)\s+([a-zA-Z]+)\s+([0-9.]+)\s+([a-zA-Z]+)\s+([0-9.]+)\s+([a-zA-Z]+)\]'
                    
                    matches = re.findall(pattern, content)
                    for match in matches:
                        name, lower_val, lower_unit, est_val, est_unit, upper_val, upper_unit = match
                        
                        # Convert to milliseconds for comparison
                        def to_ms(val, unit):
                            val = float(val)
                            if unit == 'ns':
                                return val / 1_000_000
                            elif unit in ['μs', 'us']:
                                return val / 1_000
                            elif unit == 'ms':
                                return val
                            elif unit == 's':
                                return val * 1_000
                            return val
                        
                        est_ms = to_ms(est_val, est_unit)
                        
                        print(f"Benchmark: {name}")
                        print(f"  Estimated time: {est_ms:.2f}ms")
                        
                        # Check against development plan thresholds
                        if est_ms > 200:  # p95 threshold
                            print(f"  ⚠️  WARNING: Exceeds 200ms threshold")
                            performance_warning = True
                        elif est_ms > 30:  # p50 threshold
                            print(f"  ℹ️  INFO: Above 30ms p50 target")
                        else:
                            print(f"  ✅ Within performance targets")
                    
                    # Set GitHub Actions output
                    if performance_warning:
                        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                            f.write("performance_warning=true\n")
                    
                    # Check for report files generated by our framework
                    if os.path.exists('target/benchmark_report.json'):
                        print("✅ Benchmark report generated successfully")
                                
            except FileNotFoundError:
                print("Benchmark output file not found")
                # Still continue - don't fail the build

        if __name__ == "__main__":
            analyze_benchmarks()
        EOF

        python3 analyze_benchmarks.py

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          target/benchmark-output.txt
          target/benchmark_report.json
          target/benchmark_report.md
        retention-days: 30

    - name: Comment benchmark results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = 'target/benchmark_report.md';
          
          if (fs.existsSync(path)) {
            const report = fs.readFileSync(path, 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## 📊 Benchmark Results\n\n${report}`
            });
          } else {
            console.log('No benchmark report found to comment');
          }

    - name: Fail on performance regression
      if: steps.benchmark_report.outputs.performance_warning == 'true'
      run: |
        echo "⚠️ Performance regression detected!"
        echo "Some benchmarks exceeded the 200ms p95 latency threshold."
        echo "Please review the benchmark results and optimize performance."
        exit 1

  # Memory usage benchmark (Linux-specific)
  memory_benchmark:
    name: Memory Usage Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Install memory monitoring tools
      run: |
        sudo apt-get update
        sudo apt-get install -y valgrind time

    - name: Cache Cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-memory-${{ hashFiles('**/Cargo.lock') }}

    - name: Build release binary for memory testing
      run: cargo build --release -p skreaver-cli

    - name: Run memory usage test
      run: |
        echo "Testing memory usage with /usr/bin/time:"
        /usr/bin/time -v ./target/release/skreaver-cli --help 2>&1 | tee memory_report.txt
        
        # Extract memory usage
        MAX_RSS=$(grep "Maximum resident set size" memory_report.txt | awk '{print $6}')
        MAX_RSS_MB=$((MAX_RSS / 1024))
        
        echo "Maximum RSS: ${MAX_RSS_MB}MB"
        
        # Check against development plan threshold (128MB for N=32 sessions)
        # For CLI help, we expect much lower usage
        if [ "$MAX_RSS_MB" -gt 64 ]; then
          echo "⚠️ WARNING: High memory usage for CLI: ${MAX_RSS_MB}MB > 64MB"
          echo "::warning::High memory usage detected: ${MAX_RSS_MB}MB"
        else
          echo "✅ Memory usage within expected limits: ${MAX_RSS_MB}MB ≤ 64MB"
        fi

    - name: Upload memory report
      uses: actions/upload-artifact@v4
      with:
        name: memory-report
        path: memory_report.txt