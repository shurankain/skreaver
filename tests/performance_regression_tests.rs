//! # Performance Regression Detection System Tests
//!
//! Comprehensive validation of the performance regression detection system including
//! baseline management, regression detection algorithms, CI integration, and edge cases.

use skreaver_testing::regression::{
    BaselineManager, PerformanceMeasurement, RegressionConfig, RegressionError,
};
use std::collections::HashMap;
use std::time::{Duration, SystemTime};
use tempfile::TempDir;

/// Test suite for baseline management functionality
#[cfg(test)]
mod baseline_tests {
    use super::*;

    #[test]
    fn test_baseline_creation_and_storage() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        let measurement = create_test_measurement("test_baseline", 1000, 10, 100);

        manager
            .update_baseline(measurement.clone())
            .expect("Failed to update baseline");

        // Verify baseline was created
        assert_eq!(manager.list_baselines().len(), 1);
        assert!(
            manager
                .list_baselines()
                .contains(&"test_baseline".to_string())
        );

        // Verify baseline file exists
        let baseline_file = temp_dir.path().join("test_baseline.json");
        assert!(baseline_file.exists(), "Baseline file should exist");

        // Verify baseline can be loaded
        let baseline = manager
            .get_baseline("test_baseline")
            .expect("Baseline should exist");
        assert_eq!(baseline.measurements.len(), 1);
        assert_eq!(baseline.benchmark_name, "test_baseline");
    }

    #[test]
    fn test_baseline_accumulation_and_limits() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Add measurements up to the limit
        for i in 0..1005 {
            let measurement = create_test_measurement("accumulation_test", 1000 + i, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        let baseline = manager
            .get_baseline("accumulation_test")
            .expect("Baseline should exist");

        // Should be limited to 1000 measurements
        assert_eq!(
            baseline.measurements.len(),
            1000,
            "Baseline should be limited to 1000 measurements"
        );

        // Should contain the most recent measurements
        let latest = baseline
            .latest_measurement()
            .expect("Should have latest measurement");
        assert_eq!(latest.mean_duration_nanos, 2004000); // 2004 microseconds converted to nanos in create_test_measurement
    }

    #[test]
    fn test_baseline_persistence_across_manager_instances() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");

        // Create baseline with first manager instance
        {
            let mut manager1 =
                BaselineManager::new(temp_dir.path()).expect("Failed to create manager");
            let measurement = create_test_measurement("persist_test", 1500, 20, 200);
            manager1
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Load with second manager instance
        {
            let manager2 = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");
            assert_eq!(manager2.list_baselines().len(), 1);

            let baseline = manager2
                .get_baseline("persist_test")
                .expect("Baseline should exist");
            assert_eq!(baseline.benchmark_name, "persist_test");
            assert_eq!(baseline.measurements.len(), 1);
        }
    }

    #[test]
    fn test_baseline_export_import() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Create baseline
        let measurement = create_test_measurement("export_test", 2000, 50, 500);
        manager
            .update_baseline(measurement)
            .expect("Failed to update baseline");

        // Export baseline
        let export_path = temp_dir.path().join("exported_baseline.json");
        manager
            .export_baseline("export_test", &export_path)
            .expect("Failed to export baseline");

        assert!(export_path.exists(), "Export file should exist");

        // Clear manager and import
        let temp_dir2 = TempDir::new().expect("Failed to create temp directory");
        let mut manager2 =
            BaselineManager::new(temp_dir2.path()).expect("Failed to create manager");

        let imported_name = manager2
            .import_baseline(&export_path)
            .expect("Failed to import baseline");

        assert_eq!(imported_name, "export_test");
        assert_eq!(manager2.list_baselines().len(), 1);

        let imported_baseline = manager2
            .get_baseline("export_test")
            .expect("Baseline should exist");
        assert_eq!(imported_baseline.measurements.len(), 1);
    }

    #[test]
    fn test_baseline_statistics_calculation() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Add consistent measurements for statistical analysis
        let base_duration = 1000;
        for i in 0..20 {
            let variation = (i % 10) as u64; // Small variations
            let measurement =
                create_test_measurement("stats_test", base_duration + variation, 5, 50);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        let baseline = manager
            .get_baseline("stats_test")
            .expect("Baseline should exist");
        let stats = baseline.calculate_baseline_stats(15);

        assert_eq!(stats.sample_count, 15);
        assert!(stats.mean_duration_nanos > 0);
        // Standard deviation is always non-negative by definition, removing useless comparison
        assert!(stats.min_duration_nanos <= stats.max_duration_nanos);
    }

    #[test]
    fn test_custom_regression_config() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");

        let strict_config = RegressionConfig {
            mean_threshold_percent: 5.0, // Very strict
            p95_threshold_percent: 7.0,
            p99_threshold_percent: 10.0,
            min_samples: 5,
            use_statistical_test: false,
            significance_level: 0.01,
        };

        let mut manager = BaselineManager::with_config(temp_dir.path(), strict_config)
            .expect("Failed to create manager");

        // Build baseline
        for _ in 0..10 {
            let measurement = create_test_measurement("strict_test", 1000, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Test with 6% performance degradation (should trigger with strict config)
        let test_measurement = create_test_measurement("strict_test", 1060, 15, 150);
        let analysis = manager
            .detect_regression(&test_measurement)
            .expect("Failed to detect regression");

        assert!(
            analysis.is_regression,
            "Should detect regression with strict thresholds: mean_change={:.1}%, details={}",
            analysis.mean_change_percent, analysis.details
        );
        assert!(
            analysis.mean_change_percent > 5.0,
            "Mean change should exceed strict threshold"
        );
    }
}

/// Test suite for regression detection algorithms
#[cfg(test)]
mod regression_detection_tests {
    use super::*;

    #[test]
    fn test_no_regression_detection() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Build stable baseline
        for i in 0..15 {
            let measurement = create_test_measurement("stable_test", 1000 + i, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Test with similar performance
        let test_measurement = create_test_measurement("stable_test", 1005, 12, 120);
        let analysis = manager
            .detect_regression(&test_measurement)
            .expect("Failed to analyze performance");

        assert!(
            !analysis.is_regression,
            "Should not detect regression for stable performance"
        );
        assert!(
            analysis.mean_change_percent < 10.0,
            "Change should be within acceptable range"
        );
        assert!(
            analysis.details.contains("acceptable thresholds"),
            "Details should indicate acceptable performance"
        );
    }

    #[test]
    fn test_mean_regression_detection() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Build consistent baseline
        for _ in 0..15 {
            let measurement = create_test_measurement("mean_regression_test", 1000, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Test with significant mean increase (20% worse to ensure detection)
        let test_measurement = create_test_measurement("mean_regression_test", 1200, 15, 150);
        let analysis = manager
            .detect_regression(&test_measurement)
            .expect("Failed to analyze performance");

        assert!(analysis.is_regression, "Should detect mean regression");
        assert!(
            analysis.mean_change_percent > 10.0,
            "Mean change should exceed threshold"
        );
        assert!(
            analysis.details.contains("Mean exceeded threshold"),
            "Details should mention mean regression"
        );
    }

    #[test]
    fn test_p95_regression_detection() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Build baseline with low variance
        for _ in 0..15 {
            let measurement = create_test_measurement("p95_test", 1000, 5, 50);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Test with high variance (worse P95)
        let test_measurement = create_test_measurement("p95_test", 1050, 100, 1000);
        let analysis = manager
            .detect_regression(&test_measurement)
            .expect("Failed to analyze performance");

        if analysis.is_regression {
            assert!(analysis.p95_change_percent > 15.0 || analysis.mean_change_percent > 10.0);
        }
    }

    #[test]
    fn test_insufficient_baseline_data() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Add only a few measurements (less than min_samples)
        for i in 0..3 {
            let measurement = create_test_measurement("sparse_test", 1000 + i, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Test with much worse performance (should not trigger due to insufficient data)
        let test_measurement = create_test_measurement("sparse_test", 2000, 50, 500);
        let analysis = manager
            .detect_regression(&test_measurement)
            .expect("Failed to analyze performance");

        assert!(
            !analysis.is_regression,
            "Should not flag regression with insufficient baseline data"
        );
        assert!(
            analysis.details.contains("Insufficient baseline data"),
            "Details should mention insufficient data"
        );
    }

    #[test]
    fn test_multiple_threshold_regression() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Build baseline
        for _ in 0..15 {
            let measurement = create_test_measurement("multi_threshold_test", 1000, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Test with performance that triggers multiple thresholds
        let test_measurement = create_test_measurement("multi_threshold_test", 1300, 200, 2000);
        let analysis = manager
            .detect_regression(&test_measurement)
            .expect("Failed to analyze performance");

        assert!(analysis.is_regression, "Should detect regression");

        // Should trigger multiple thresholds
        let violations = analysis.details.split(';').count();
        assert!(
            violations > 1,
            "Should trigger multiple threshold violations"
        );
    }

    #[test]
    fn test_baseline_not_found_error() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        let test_measurement = create_test_measurement("nonexistent_test", 1000, 10, 100);
        let result = manager.detect_regression(&test_measurement);

        assert!(
            result.is_err(),
            "Should return error for nonexistent baseline"
        );
        match result.unwrap_err() {
            RegressionError::BaselineNotFound(_) => {
                // Expected error type
            }
            other => panic!("Expected BaselineNotFound error, got: {:?}", other),
        }
    }
}

/// Test suite for CI integration scenarios
#[cfg(test)]
mod ci_integration_tests {
    use super::*;
    use std::env;

    #[test]
    fn test_ci_environment_simulation() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");

        // Simulate CI environment variables
        unsafe {
            env::set_var("CI", "true");
            env::set_var("GITHUB_SHA", "abc123def456");
            env::set_var("GITHUB_REF", "refs/heads/main");
        }

        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Create enough baseline measurements for statistical validity
        for i in 0..15 {
            let mut baseline_measurement = create_test_measurement("ci_test", 1000 + i, 10, 100);
            baseline_measurement.commit_hash = Some(format!("previous_commit_{}", i));
            baseline_measurement.branch = Some("main".to_string());
            manager
                .update_baseline(baseline_measurement)
                .expect("Failed to update baseline");
        }

        // Simulate current CI run
        let mut current_measurement = create_test_measurement("ci_test", 1200, 15, 150);
        current_measurement.commit_hash = Some("abc123def456".to_string());
        current_measurement.branch = Some("main".to_string());

        let analysis = manager
            .detect_regression(&current_measurement)
            .expect("Failed to detect regression");

        assert!(
            analysis.is_regression,
            "Should detect CI regression: mean_change={:.1}%, details={}",
            analysis.mean_change_percent, analysis.details
        );

        // Clean up environment
        unsafe {
            env::remove_var("CI");
            env::remove_var("GITHUB_SHA");
            env::remove_var("GITHUB_REF");
        }
    }

    #[test]
    fn test_ci_matrix_configuration_handling() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Test different matrix configurations
        let configurations = vec![
            ("rust-stable", "ubuntu-latest"),
            ("rust-beta", "ubuntu-latest"),
            ("rust-stable", "macos-latest"),
        ];

        for (rust_version, os) in configurations {
            let benchmark_name = format!("ci_matrix_{}_{}", rust_version, os);

            // Simulate baseline for this configuration
            let measurement = create_test_measurement(&benchmark_name, 1000, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");

            // Verify baseline exists for this configuration
            assert!(manager.get_baseline(&benchmark_name).is_some());
        }

        // Verify all configurations were stored
        assert_eq!(manager.list_baselines().len(), 3);

        // Test loading from disk - create new manager instance
        let manager2 = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");
        assert_eq!(manager2.list_baselines().len(), 3);
    }

    #[test]
    fn test_ci_artifact_storage_simulation() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let artifacts_dir = temp_dir.path().join("artifacts");
        std::fs::create_dir_all(&artifacts_dir).expect("Failed to create artifacts dir");

        let mut manager = BaselineManager::new(&artifacts_dir).expect("Failed to create manager");

        // Simulate storing baseline as CI artifact
        let measurement = create_test_measurement("artifact_test", 1500, 20, 200);
        manager
            .update_baseline(measurement)
            .expect("Failed to update baseline");

        // Export baseline for artifact storage
        let artifact_path = artifacts_dir.join("baseline_artifact.json");
        manager
            .export_baseline("artifact_test", &artifact_path)
            .expect("Failed to export artifact");

        assert!(artifact_path.exists(), "Artifact file should exist");

        // Simulate loading from artifact in different CI job
        let temp_dir2 = TempDir::new().expect("Failed to create temp directory");
        let mut manager2 =
            BaselineManager::new(temp_dir2.path()).expect("Failed to create manager");

        let imported_name = manager2
            .import_baseline(&artifact_path)
            .expect("Failed to import artifact");
        assert_eq!(imported_name, "artifact_test");
    }

    #[test]
    fn test_ci_performance_overhead() {
        let start_time = std::time::Instant::now();

        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Simulate typical CI workflow
        for i in 0..20 {
            let measurement = create_test_measurement(&format!("ci_perf_{}", i), 1000 + i, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Simulate regression detection for current run
        for i in 0..20 {
            let test_measurement =
                create_test_measurement(&format!("ci_perf_{}", i), 1050 + i, 12, 120);
            let _ = manager.detect_regression(&test_measurement);
        }

        let total_time = start_time.elapsed();

        // CI overhead should be under 2 minutes for typical workflow
        assert!(
            total_time.as_secs() < 120,
            "CI regression detection took too long: {}s (target: <120s)",
            total_time.as_secs()
        );

        println!("CI performance overhead: {}ms", total_time.as_millis());
    }
}

/// Test suite for edge cases and error handling
#[cfg(test)]
mod edge_case_tests {
    use super::*;
    use std::fs;

    #[test]
    fn test_corrupted_baseline_file_handling() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");

        // Create corrupted baseline file
        let corrupted_file = temp_dir.path().join("corrupted.json");
        fs::write(&corrupted_file, "invalid json content").expect("Failed to write corrupted file");

        // Manager should handle corrupted files gracefully
        let manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");
        assert_eq!(
            manager.list_baselines().len(),
            0,
            "Corrupted files should be ignored"
        );
    }

    #[test]
    fn test_missing_storage_directory() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let non_existent_path = temp_dir.path().join("missing").join("nested").join("path");

        // Manager should create missing directories
        let manager = BaselineManager::new(&non_existent_path).expect("Failed to create manager");
        assert!(
            non_existent_path.exists(),
            "Manager should create missing directories"
        );

        // Should be able to store baselines
        let mut manager = manager;
        let measurement = create_test_measurement("directory_test", 1000, 10, 100);
        manager
            .update_baseline(measurement)
            .expect("Failed to update baseline");

        assert_eq!(manager.list_baselines().len(), 1);
    }

    #[test]
    fn test_extremely_large_datasets() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Test with measurements that have extreme values
        let extreme_measurement = PerformanceMeasurement {
            benchmark_name: "extreme_test".to_string(),
            timestamp: SystemTime::now(),
            commit_hash: None,
            branch: None,
            mean_duration_nanos: u64::MAX / 2, // Very large duration
            median_duration_nanos: u64::MAX / 2,
            min_duration_nanos: u64::MAX / 4,
            max_duration_nanos: u64::MAX / 2,
            std_dev_nanos: u64::MAX / 10,
            sample_count: usize::MAX / 1000,
            throughput_ops_per_sec: Some(f64::MIN_POSITIVE),
            custom_metrics: HashMap::new(),
        };

        // Should handle extreme values without panicking
        manager
            .update_baseline(extreme_measurement.clone())
            .expect("Failed to handle extreme values");

        let analysis = manager.detect_regression(&extreme_measurement);
        assert!(
            analysis.is_ok(),
            "Should handle extreme values in regression detection"
        );
    }

    #[test]
    fn test_zero_and_negative_durations() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Create measurement with zero duration
        let zero_measurement = PerformanceMeasurement {
            benchmark_name: "zero_test".to_string(),
            timestamp: SystemTime::now(),
            commit_hash: None,
            branch: None,
            mean_duration_nanos: 0,
            median_duration_nanos: 0,
            min_duration_nanos: 0,
            max_duration_nanos: 0,
            std_dev_nanos: 0,
            sample_count: 1,
            throughput_ops_per_sec: None,
            custom_metrics: HashMap::new(),
        };

        // Should handle zero durations gracefully
        manager
            .update_baseline(zero_measurement.clone())
            .expect("Failed to handle zero duration");

        let analysis = manager.detect_regression(&zero_measurement);
        assert!(
            analysis.is_ok(),
            "Should handle zero durations in regression detection"
        );
    }

    #[test]
    fn test_concurrent_baseline_access() {
        use std::sync::{Arc, Mutex};
        use std::thread;

        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let manager = Arc::new(Mutex::new(
            BaselineManager::new(temp_dir.path()).expect("Failed to create manager"),
        ));

        let handles: Vec<_> = (0..10)
            .map(|i| {
                let manager_clone = Arc::clone(&manager);
                thread::spawn(move || {
                    let measurement = create_test_measurement(
                        &format!("concurrent_test_{}", i),
                        1000 + i,
                        10,
                        100,
                    );
                    let mut manager_lock = manager_clone.lock().unwrap();
                    manager_lock
                        .update_baseline(measurement)
                        .expect("Failed to update baseline");
                })
            })
            .collect();

        // Wait for all threads to complete
        for handle in handles {
            handle.join().expect("Thread should complete successfully");
        }

        let manager_lock = manager.lock().unwrap();
        assert_eq!(
            manager_lock.list_baselines().len(),
            10,
            "All baselines should be created"
        );
    }

    #[test]
    fn test_invalid_threshold_configuration() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");

        // Test with invalid threshold values
        let invalid_config = RegressionConfig {
            mean_threshold_percent: -5.0, // Negative threshold
            p95_threshold_percent: f64::INFINITY,
            p99_threshold_percent: f64::NAN,
            min_samples: 0, // Invalid sample count
            use_statistical_test: true,
            significance_level: 2.0, // Invalid significance level
        };

        // Manager should handle invalid config gracefully
        let manager_result = BaselineManager::with_config(temp_dir.path(), invalid_config);
        assert!(
            manager_result.is_ok(),
            "Should handle invalid config gracefully"
        );
    }
}

/// Test suite for security aspects
#[cfg(test)]
mod security_tests {
    use super::*;
    use std::fs;

    #[test]
    fn test_malicious_baseline_file_content() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");

        // Create file with malicious JSON content
        let malicious_file = temp_dir.path().join("malicious.json");
        let malicious_content = r#"{
            "benchmark_name": "malicious",
            "measurements": [],
            "created_at": "../../etc/passwd",
            "updated_at": "<script>alert('xss')</script>"
        }"#;

        fs::write(&malicious_file, malicious_content).expect("Failed to write malicious file");

        // Manager should handle malicious content safely
        let manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");
        // Malicious file should be ignored due to parsing error
        assert_eq!(manager.list_baselines().len(), 0);
    }

    #[test]
    fn test_path_traversal_protection() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Try to create baseline with path traversal in name
        let traversal_measurement = PerformanceMeasurement {
            benchmark_name: "../../../etc/passwd".to_string(),
            timestamp: SystemTime::now(),
            commit_hash: None,
            branch: None,
            mean_duration_nanos: 1_000_000,
            median_duration_nanos: 1_000_000,
            min_duration_nanos: 900_000,
            max_duration_nanos: 1_100_000,
            std_dev_nanos: 50_000,
            sample_count: 100,
            throughput_ops_per_sec: None,
            custom_metrics: HashMap::new(),
        };

        // Should be handled safely without creating files outside temp_dir
        manager
            .update_baseline(traversal_measurement)
            .expect("Should handle path traversal safely");

        // Verify no files were created outside temp_dir
        let system_passwd = std::path::Path::new("/etc/passwd");
        let temp_passwd = temp_dir.path().join("passwd");

        assert!(
            !temp_passwd.exists()
                || !system_passwd.exists()
                || fs::metadata(system_passwd).unwrap().modified().unwrap()
                    < SystemTime::now() - Duration::from_secs(60),
            "Should not modify system files"
        );
    }

    #[test]
    fn test_input_validation_and_sanitization() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Test with various malicious inputs
        let long_name = "a".repeat(1000); // Reduced from 10000 to avoid filesystem issues
        let malicious_inputs = vec![
            "'; DROP TABLE baselines; --",
            "<script>alert('xss')</script>",
            "binary_data_with_special_chars", // Simplified to avoid null bytes
            &long_name,
        ];

        for malicious_input in malicious_inputs {
            let measurement = PerformanceMeasurement {
                benchmark_name: malicious_input.to_string(),
                timestamp: SystemTime::now(),
                commit_hash: Some(malicious_input.to_string()),
                branch: Some(malicious_input.to_string()),
                mean_duration_nanos: 1_000_000,
                median_duration_nanos: 1_000_000,
                min_duration_nanos: 900_000,
                max_duration_nanos: 1_100_000,
                std_dev_nanos: 50_000,
                sample_count: 100,
                throughput_ops_per_sec: None,
                custom_metrics: {
                    let mut metrics = HashMap::new();
                    metrics.insert(malicious_input.to_string(), 123.45);
                    metrics
                },
            };

            // Should handle malicious inputs safely - very long names might fail at filesystem level
            let result = manager.update_baseline(measurement);
            if malicious_input.len() > 255 {
                // Very long filenames may fail at filesystem level, which is acceptable
                // The system should not crash or have security vulnerabilities
                if result.is_err() {
                    println!(
                        "Long filename rejected by filesystem: {} chars",
                        malicious_input.len()
                    );
                }
            } else {
                assert!(
                    result.is_ok(),
                    "Should handle malicious input safely: {}",
                    malicious_input
                );
            }
        }
    }
}

/// Test suite for performance validation
#[cfg(test)]
mod performance_validation_tests {
    use super::*;

    #[test]
    fn test_baseline_operation_performance() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        let start_time = std::time::Instant::now();

        // Test baseline creation performance
        for i in 0..100 {
            let measurement =
                create_test_measurement(&format!("perf_test_{}", i), 1000 + i, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        let creation_time = start_time.elapsed();

        // Test regression detection performance
        let detection_start = std::time::Instant::now();

        for i in 0..100 {
            let test_measurement =
                create_test_measurement(&format!("perf_test_{}", i), 1100 + i, 15, 150);
            let _ = manager.detect_regression(&test_measurement);
        }

        let detection_time = detection_start.elapsed();

        println!("Performance validation results:");
        println!(
            "  Baseline creation: {}ms for 100 operations",
            creation_time.as_millis()
        );
        println!(
            "  Regression detection: {}ms for 100 operations",
            detection_time.as_millis()
        );

        // Performance requirements based on project targets
        assert!(
            creation_time.as_millis() < 5000,
            "Baseline creation too slow: {}ms (target: <5000ms)",
            creation_time.as_millis()
        );
        assert!(
            detection_time.as_millis() < 3000,
            "Regression detection too slow: {}ms (target: <3000ms)",
            detection_time.as_millis()
        );
    }

    #[test]
    fn test_memory_usage_efficiency() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Create large baseline to test memory efficiency
        for i in 0..2000 {
            let measurement = create_test_measurement(
                "memory_test",
                1000 + (i % 100),
                10 + (i % 5),
                (100 + (i % 20)) as usize,
            );
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        // Verify baseline is limited to 1000 measurements (memory efficiency)
        let baseline = manager
            .get_baseline("memory_test")
            .expect("Baseline should exist");
        assert_eq!(
            baseline.measurements.len(),
            1000,
            "Baseline should be limited for memory efficiency"
        );

        // Test that multiple baselines can coexist
        for i in 0..10 {
            let measurement = create_test_measurement(&format!("memory_test_{}", i), 1000, 10, 100);
            manager
                .update_baseline(measurement)
                .expect("Failed to update baseline");
        }

        assert_eq!(
            manager.list_baselines().len(),
            11,
            "Multiple baselines should coexist"
        );
    }

    #[test]
    fn test_algorithm_accuracy_validation() {
        let temp_dir = TempDir::new().expect("Failed to create temp directory");
        let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

        // Create precise baseline with known statistics
        let baseline_durations = [1000, 1010, 1020, 1030, 1040]; // Mean: 1020, StdDev: ~15.8

        for duration in baseline_durations.iter() {
            for _ in 0..3 {
                // Multiple measurements per duration for statistical validity
                let measurement = create_test_measurement("accuracy_test", *duration, 2, 20);
                manager
                    .update_baseline(measurement)
                    .expect("Failed to update baseline");
            }
        }

        // Test known regression case
        let regression_measurement = create_test_measurement("accuracy_test", 1150, 5, 50); // ~12.7% increase
        let analysis = manager
            .detect_regression(&regression_measurement)
            .expect("Failed to detect regression");

        assert!(analysis.is_regression, "Should detect known regression");
        assert!(
            (analysis.mean_change_percent - 12.7).abs() < 2.0,
            "Mean change calculation should be accurate: expected ~12.7%, got {:.1}%",
            analysis.mean_change_percent
        );

        // Test known non-regression case
        let stable_measurement = create_test_measurement("accuracy_test", 1025, 3, 30); // ~0.5% increase
        let stable_analysis = manager
            .detect_regression(&stable_measurement)
            .expect("Failed to analyze stable performance");

        assert!(
            !stable_analysis.is_regression,
            "Should not detect regression for stable performance"
        );
        assert!(
            stable_analysis.mean_change_percent < 1.0,
            "Stable performance should show minimal change: {:.1}%",
            stable_analysis.mean_change_percent
        );
    }
}

// Helper function to create test measurements
fn create_test_measurement(
    name: &str,
    mean_micros: u64,
    std_dev_micros: u64,
    sample_count: usize,
) -> PerformanceMeasurement {
    use skreaver_testing::benchmarks::BenchmarkResult;

    let result = BenchmarkResult {
        name: name.to_string(),
        iterations: sample_count,
        mean: Duration::from_micros(mean_micros),
        median: Duration::from_micros(mean_micros),
        min: Duration::from_micros(mean_micros - std_dev_micros),
        max: Duration::from_micros(mean_micros + std_dev_micros),
        std_dev: Duration::from_micros(std_dev_micros),
        throughput: None,
        total_operations: None,
    };

    PerformanceMeasurement::from(result)
}

/// Integration test combining all components
#[test]
fn test_complete_regression_detection_workflow() {
    let temp_dir = TempDir::new().expect("Failed to create temp directory");
    let mut manager = BaselineManager::new(temp_dir.path()).expect("Failed to create manager");

    // Phase 1: Establish baseline from "historical" runs
    println!("Phase 1: Establishing baselines...");
    for i in 0..20 {
        let measurement = create_test_measurement("integration_test", 1000 + i, 10, 100);
        manager
            .update_baseline(measurement)
            .expect("Failed to establish baseline");
    }

    // Phase 2: Test current performance (no regression)
    println!("Phase 2: Testing stable performance...");
    let stable_measurement = create_test_measurement("integration_test", 1010, 12, 120);
    let stable_analysis = manager
        .detect_regression(&stable_measurement)
        .expect("Failed to analyze stable performance");

    assert!(
        !stable_analysis.is_regression,
        "Stable performance should not trigger regression"
    );
    println!("  âœ“ Stable performance correctly identified");

    // Phase 3: Test regression detection
    println!("Phase 3: Testing regression detection...");
    let regression_measurement = create_test_measurement("integration_test", 1200, 20, 200);
    let regression_analysis = manager
        .detect_regression(&regression_measurement)
        .expect("Failed to detect regression");

    assert!(
        regression_analysis.is_regression,
        "Performance regression should be detected"
    );
    println!("  âœ“ Performance regression correctly detected");
    println!(
        "  â†³ Mean change: {:.1}%",
        regression_analysis.mean_change_percent
    );
    println!(
        "  â†³ P95 change: {:.1}%",
        regression_analysis.p95_change_percent
    );

    // Phase 4: Test export/import workflow (simulating CI artifacts)
    println!("Phase 4: Testing CI artifact workflow...");
    let export_path = temp_dir.path().join("ci_baseline.json");
    manager
        .export_baseline("integration_test", &export_path)
        .expect("Failed to export baseline");

    let temp_dir2 = TempDir::new().expect("Failed to create temp directory");
    let mut manager2 =
        BaselineManager::new(temp_dir2.path()).expect("Failed to create second manager");

    let imported_name = manager2
        .import_baseline(&export_path)
        .expect("Failed to import baseline");
    assert_eq!(imported_name, "integration_test");

    // Verify imported baseline works for regression detection
    let imported_analysis = manager2
        .detect_regression(&regression_measurement)
        .expect("Failed to use imported baseline");
    assert!(
        imported_analysis.is_regression,
        "Imported baseline should detect regression"
    );
    println!("  âœ“ CI artifact workflow successful");

    // Phase 5: Performance validation
    println!("Phase 5: Validating performance requirements...");
    let perf_start = std::time::Instant::now();
    for _ in 0..100 {
        let _ = manager.detect_regression(&stable_measurement);
    }
    let perf_time = perf_start.elapsed();

    assert!(
        perf_time.as_millis() < 1000,
        "100 regression detections should complete in <1s"
    );
    println!(
        "  âœ“ Performance requirements met: {}ms for 100 detections",
        perf_time.as_millis()
    );

    println!("\nðŸŽ‰ Complete regression detection workflow test passed!");
}
